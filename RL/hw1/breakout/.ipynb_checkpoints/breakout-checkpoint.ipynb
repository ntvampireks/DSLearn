{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7d61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import DuelCNN\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39d5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps'\n",
    "torch.manual_seed(113)\n",
    "random.seed(113)\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, max_memory, batch_size, lr, epsilon, epsilon_rate, gamma, iterations):\n",
    "\n",
    "        self.state_size_h = env.observation_space.shape[0]\n",
    "        self.state_size_w = env.observation_space.shape[1]\n",
    "        self.state_size_c = env.observation_space.shape[2]\n",
    "\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        # результирующее изображение\n",
    "        self.target_h = 84 # Height after process\n",
    "        self.target_w = 84  # Widht after process\n",
    "        self.crop_dim = [30, -16 , 0, self.state_size_w]  # Cut\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_rate = epsilon_rate\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.gamma = gamma  # Коэффициент обесценивания\n",
    "        self.lr = lr\n",
    "\n",
    "        self.n_games = 0\n",
    "\n",
    "        self.max_memory = max_memory # максимальный объем памяти\n",
    "        self.memory = deque(maxlen=self.max_memory)\n",
    "\n",
    "        self.model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        lf = lambda x: (1 - x / (iterations - 1)) * (1 - 0.01) + 0.01\n",
    "        self.sheduler = lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lf)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        plt.imshow(frame)\n",
    "        frame = frame / 255\n",
    "        return frame\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > self.max_memory * 0.2:\n",
    "            # случайно выбираем пачку из памяти\n",
    "            mini_sample = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "            actions = np.array(actions)\n",
    "            dones = np.array(dones)\n",
    "            return self.train_step(states, actions, rewards, next_states, dones)\n",
    "        return 0, 0\n",
    "        #else:\n",
    "        #    # или используем всю память\n",
    "        #    mini_sample = self.memory\n",
    "\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)#.unsqueeze(-1)\n",
    "\n",
    "        done = torch.tensor(done, dtype=torch.float,device=DEVICE)\n",
    "        # получили функцию ценности действий для текущего состояния/\n",
    "        # оценка текущего состояния\n",
    "        pred_q = self.model(state)\n",
    "        pred_q_next_states = self.model(next_state)\n",
    "        target = self.target_model(next_state)\n",
    "\n",
    "        # выбрать полезности по номеру совершенного действия\n",
    "        selected_q_value = pred_q.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # выбрать номера действий по результатам следующего состояния\n",
    "        r = pred_q_next_states.max(1)[1]\n",
    "        # из таргета выбираем те полезности которые соответствуют действиям предсказанным для следующего состояния\n",
    "        next_states_target_q_value = target.gather(1, r.unsqueeze(1)).squeeze(1)\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)  # применили формулу беллмана\n",
    "\n",
    "        #кусок если хотим работать с полнымит векторами действий вида BxN_действий\n",
    "        #max_q = torch.max(pred_q_next_states, -1)[0].unsqueeze(1)\n",
    "        #td = reward + self.gamma * max_q * (1-done)\n",
    "        #td = torch.where(done, reward, td)\n",
    "        # для все action выбранных на текущих состояниях - корректируем их полезности на td Q-Q_\n",
    "        #actions_to_correct_Q = torch.argmax(action, axis=-1).unsqueeze(1)\n",
    "        #target = target.scatter(1, actions_to_correct_Q, td)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        #loss = self.criterion(pred_q, target.detach())\n",
    "        loss = self.criterion(selected_q_value, expected_q_value.detach())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), torch.max(pred_q).item()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) > self.epsilon:\n",
    "            state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "            q_values = self.model(state)  # (1, action_size)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_size)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db34d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations, lr, memory_size, batch_size, epsilon, epsilon_rate, gamma):\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    plot_mean_100 = []\n",
    "    num_steps_by_game = []\n",
    "\n",
    "    total_score = 0\n",
    "    record = -20\n",
    "    score = 0\n",
    "    total_step = 0\n",
    "    episode_steps = 0\n",
    "    env = gym.make(\"Breakout-v4\")#, render_mode='human')\n",
    "\n",
    "    agent = Agent(env, memory_size, batch_size, lr, epsilon, epsilon_rate, gamma, iterations)\n",
    "    #agent.model = torch.load(\"./model/model.pth\")\n",
    "    #agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "    #agent.target_model.eval()\n",
    "\n",
    "    agent\n",
    "    state, _= env.reset(seed=113)\n",
    "    #plt.imshow(state)\n",
    "    state = agent.preProcess(state)\n",
    "    state = np.stack((state, state, state, state)) ## так как не знаем вектора движения - то даем последовательность кадров\n",
    "\n",
    "    startTime = time.time()\n",
    "    total_max_q_val = 0  # Total max q vals # Total reward for each episode\n",
    "    total_loss = 0\n",
    "    last_100_ep_reward = deque(maxlen=100)\n",
    "\n",
    "    while agent.n_games < iterations:\n",
    "\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        next_state = agent.preProcess(next_state)\n",
    "        next_state = np.stack((next_state, state[0], state[1], state[2])) #новый кадр-состояние в начало\n",
    "        # если хотим работать с полными векторами действий\n",
    "        act_full = np.zeros(agent.action_size)\n",
    "        act_full[action] = 1\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        episode_steps += 1\n",
    "        total_step += 1\n",
    "\n",
    "\n",
    "        if total_step % 1000 == 0:\n",
    "            if agent.epsilon > 0.07:\n",
    "                agent.epsilon = agent.epsilon * epsilon_rate\n",
    "            agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "\n",
    "        if episode_steps % 4 == 0:\n",
    "            loss, max_q_val = agent.train_long_memory()\n",
    "            total_loss += loss\n",
    "            total_max_q_val += max_q_val\n",
    "\n",
    "        if done:\n",
    "            currentTime = time.time()  # Keep current time\n",
    "            time_passed = currentTime - startTime\n",
    "            agent.sheduler.step()\n",
    "            agent.n_games += 1\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            num_steps_by_game.append(episode_steps)\n",
    "\n",
    "            last_100_ep_reward.append(score)\n",
    "            avg_100 = sum(last_100_ep_reward) / len(last_100_ep_reward)\n",
    "            plot_mean_100.append(avg_100)\n",
    "            avg_max_q_val = total_max_q_val / episode_steps\n",
    "            print('Game', agent.n_games, 'Score', score,\n",
    "                  f'Loss: {total_loss:.2f}',\n",
    "                  f\"Mean_score:{mean_score: .2f}\",\n",
    "                  f'Mean_by_100: {avg_100:.2f}',              #\"TotalScore:\", total_score,\n",
    "                  f'Duration: {time_passed:.2f}',\n",
    "                  f'Epsilon: {agent.epsilon: .2f}',\n",
    "                  \"Episode_steps:\", episode_steps,\n",
    "                  \"Total_steps:\", total_step,\n",
    "                  f'avg_q_val: {avg_max_q_val:.2f}', )\n",
    "\n",
    "            score = 0\n",
    "            episode_steps = 0\n",
    "            total_max_q_val = 0  # Total max q vals\n",
    "            total_loss = 0\n",
    "\n",
    "            state, _ = env.reset(seed=113)\n",
    "            state = agent.preProcess(state)\n",
    "            state = np.stack((state, state, state, state))\n",
    "\n",
    "            agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "            startTime = time.time()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    x = np.arange(agent.n_games)\n",
    "    ax.plot(x, plot_scores, label=\"Reward\")\n",
    "    ax.plot(x, plot_mean_scores, label=\"Mean_reward\")\n",
    "    ax.plot(x, plot_mean_100, label=\"Mean_reward_Last100\")\n",
    "    ax.legend()\n",
    "    fig.savefig('plot_breakout.png')\n",
    "\n",
    "    with open(r'./reward.txt', 'w') as fp:\n",
    "        for item in plot_scores:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "        print('Done')\n",
    "\n",
    "    with open(r'./mean_reward.txt', 'w') as fp:\n",
    "        for item in plot_mean_scores:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2970f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(iterations=1500, lr=0.001, memory_size=50000, batch_size=64, epsilon=0.95, epsilon_rate=0.99, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c694c7",
   "metadata": {},
   "source": [
    "![Динамика обучения](plot_breakout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95def5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "def show_video(folder=\"./video\"):\n",
    "    mp4list = glob.glob(folder + '/*.mp4')\n",
    "    print (mp4list)\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "import glob\n",
    "\n",
    "\n",
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')\n",
    "vid = video_recorder.VideoRecorder(env=env, path=\"./video/vid.mp4\")\n",
    "\n",
    "done = False\n",
    "\n",
    "agent = Agent(env, 50000, 64, 0.00001, 0.05, 0.99, 0.97)\n",
    "\n",
    "state, _ = env.reset(seed=113)\n",
    "state = agent.preProcess(state)\n",
    "state = np.stack((state, state, state, state))\n",
    "agent.model = torch.load(\"./model/model.pth\")\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info, _ = env.step(action)\n",
    "    next_state = agent.preProcess(next_state)\n",
    "    next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "    #startrender = time.time()\n",
    "    vid.capture_frame()\n",
    "    #print (\"env.step \" , (stopsim - startsim))\n",
    "    #print (\"env.render \" , (stoprender - startrender))\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break;\n",
    "env.close()\n",
    "vid.close()\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
