{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIvhwE4zxX0s",
    "outputId": "dfa3b3dd-38a1-4a67-879f-46376640ef14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/vampire/miniforge3/lib/python3.9/site-packages (3.6.7)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.2.0.tar.gz (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bokeh\n",
      "  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting umap-learn\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from nltk) (2022.6.2)\n",
      "Requirement already satisfied: click in /Users/vampire/miniforge3/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/vampire/miniforge3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/vampire/miniforge3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from gensim) (1.8.0)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado>=5.1 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (3.1.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (4.3.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from umap-learn) (1.0.2)\n",
      "Collecting numba>=0.49\n",
      "  Downloading numba-0.55.2-cp39-cp39-macosx_11_0_arm64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh) (2.1.1)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.1-cp39-cp39-macosx_11_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/vampire/miniforge3/lib/python3.9/site-packages (from numba>=0.49->umap-learn) (62.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from packaging>=16.8->bokeh) (3.0.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vampire/miniforge3/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Building wheels for collected packages: gensim, umap-learn, pynndescent\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-4.2.0-cp39-cp39-macosx_11_0_arm64.whl size=23932528 sha256=b1738c2ec6c77c858120aa39949946830afe8df6278f00b5b2961fda037b8884\n",
      "  Stored in directory: /Users/vampire/Library/Caches/pip/wheels/ed/5e/79/d2997e72ba8900a820dd5870a3566779e52ee8279f71b4c799\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=246a5bb13b9ff0d236d1e8f4f84945d73ad42b280075be8aaacb728bbd741efc\n",
      "  Stored in directory: /Users/vampire/Library/Caches/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=32ca7268693164f8df9021212e4957c85e3055af4654eada1b719cf26d15753f\n",
      "  Stored in directory: /Users/vampire/Library/Caches/pip/wheels/5b/f5/6e/aac11d69fe2115d9ac871d6c148b361f0d3f8a35ed7354fa03\n",
      "Successfully built gensim umap-learn pynndescent\n",
      "Installing collected packages: smart_open, nltk, llvmlite, numba, gensim, bokeh, pynndescent, umap-learn\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.7\n",
      "    Uninstalling nltk-3.6.7:\n",
      "      Successfully uninstalled nltk-3.6.7\n",
      "Successfully installed bokeh-2.4.3 gensim-4.2.0 llvmlite-0.38.1 nltk-3.7 numba-0.55.2 pynndescent-0.5.7 smart_open-6.0.0 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "#Установка нужных пакетов\n",
    "!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hF9WPCtfxZR9",
    "outputId": "0d73ae54-58c4-4d44-bcc7-8d35c800f08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\r\n"
     ]
    }
   ],
   "source": [
    "# выгружаем датасет:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MaFpN9pvxtNg",
    "outputId": "f8255b0e-bdfc-47cf-fa91-7f9075b78d14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvXRbOKGx0l_",
    "outputId": "f265a2aa-fc44-4763-bc87-fe05ee0a7203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data[50]\n",
    "print(tokenizer.tokenize(data[50]))\n",
    "r = tokenizer.tokenize(data[50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4APbZkpElkGk",
    "outputId": "34b2c234-4500-4ca5-feb9-5f3e4a82a07b"
   },
   "outputs": [],
   "source": [
    "data_tok = list()\n",
    "for i in data:\n",
    "    data_tok.append([token.lower() for token in tokenizer.tokenize(i)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkxi_QOySCl"
   },
   "source": [
    "#Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [],
   "source": [
    "#data_tok = tokenizer.tokenize(data.lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#checking\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtKeoLCYzY4j"
   },
   "source": [
    "###Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "a_BxzSv9yR0w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/vampire/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "can        73726\n",
      "i         149868\n",
      "get        26467\n",
      "back        3167\n",
      "with       29149\n",
      "...          ...\n",
      "navicat        1\n",
      "betweed        1\n",
      "newshunt       1\n",
      "debeak         1\n",
      "cockerel       1\n",
      "\n",
      "[80304 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dict_words = {}\n",
    "\n",
    "for token in data_tok:\n",
    "    for word in token: \n",
    "        word_lemmatizing = lemmatizer.lemmatize(word)\n",
    "        if word_lemmatizing in dict_words.keys(): \n",
    "            dict_words[word_lemmatizing] += 1\n",
    "        else:\n",
    "            dict_words[word_lemmatizing] = 1\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(dict_words, orient='index')\n",
    "df\n",
    "df.sort_values(0, ascending = False).head(10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1SM3sn1zf1b"
   },
   "source": [
    "###Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Q88BIteDzpWR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/vampire/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vampire/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80304\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lisss = list()\n",
    "for l in data_tok:\n",
    "    for w in l:\n",
    "        lisss.append(lemmatizer.lemmatize(w))\n",
    "print(len(set(lisss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxKa8yUUzqNN"
   },
   "source": [
    "###Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "x91DX51qzszR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67026\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lisss2 = list()\n",
    "for l in data_tok:\n",
    "    for w in l:\n",
    "        lisss2.append(stemmer.stem(w))\n",
    "print(len(set(lisss2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXA7Fe_izuqh"
   },
   "source": [
    "###Задание 5: Подсчитайте количество разных слов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "BGgmHzUAzwqO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7131337\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for l in data_tok:\n",
    "    for w in l:\n",
    "        i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9iloRCVShn"
   },
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex \n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "fd21b30e-aadf-47eb-e776-ba59ff427d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 21), match='https://www.wikipedia'>\n",
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'.*wikipedia*'\n",
    "test_string = 'https://www.wikipedia.org'\n",
    "result = re.search(pattern, test_string)\n",
    "print(result)\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###Задание 6: \n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "###Задание 7: \n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "###Задание 8: \n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "###Задание 9: \n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 21), match='https://www.wikipedia'>\n",
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "import re\n",
    "\n",
    "pattern = r'.*wikipedia*'\n",
    "test_string = 'https://www.wikipedia.org'\n",
    "result = re.search(pattern, test_string)\n",
    "print(result)\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='fd df r'>\n",
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "pattern = r'(\\S{2}\\s){2}\\S{2}'\n",
    "test_string = 'fd df re'\n",
    "result = re.search(pattern, test_string)\n",
    "print(result)\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Search unsuccessful.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "pattern = r'^[0-9]{1}[a-z0-9]{0,4}\\.$'\n",
    "test_string = '1qazq.2w'\n",
    "result = re.search(pattern, test_string)\n",
    "print(result)\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 9), match='any'>\n",
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "pattern = r'\\b[aeiouAEIOU]{1}[a-zA-Z]*\\b'\n",
    "test_string = 'Found any match?'\n",
    "result = re.search(pattern, test_string)\n",
    "print(result)\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csv2YN2IRXJB"
   },
   "source": [
    "Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8YWG3JhSFeZ",
    "outputId": "325b4fa5-7d56-4993-b4d0-1aae78bad94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ1qx05Q46b"
   },
   "source": [
    "Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tew2nQN4OCiW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
